Agent-Environment Framework

The environment is the setting that the agent is interacting with
in order to solve some sort of problem that exists in that environment.

The agent receives percepts through sensors, which we will abstract away for the sake of this class.
The agent function handles decision-making.
The agent uses actautors to affect the environment.

Performance: The client or stakeholders must agree on a performance measure to ensure the agent meets their objectives.
Environment: The agent must understand all relevant aspects of the environment to achieve its objective.
Actuators: The tools available to the agent to perform actions.
Sensors: The tools available to the agent to perceive the environment.

Environment Properties:
    Observability (fully vs partial) ex: a chess bot has a fully observable environment; a bot playing a strategy game with fog of war has only partial visibility.
    Deterministic vs non Deterministic (Stochastic) - refers to the effects of the agent's actions. in chess, a move is completely deterministic; a legal move will always behave the same way. However, if it's a ROBOT moving a physical chess piece in the real world, there is some non-determinism; what if the piece got dropped?
    Episodic vs sequential - episodic is when every agent action is isolated in a single instance. sequential is when the agent recevies more information in the same instance of the environment after each action.
    static vs dynamic - does the environment change or not *when the agent isn't doing anything*? ex: a chess board is static, a road is dynamic.
    discrete vs continuous - integers vs decimal numbers.
    single vs multiple agent - how many agents are in the environment? ex: chess is a 2-agent environment. You could also just consider these part of the dynamic environment.
    known vs unknown - "what does this button do?" the agent might need to do some "button mashing" to learn what does what. Account for unknowns when describing an environment

Ideal environment for ease of development: fully observable, deterministic, episodic, static, discrete, single-agent, with known variables. We try to abstract environments into this state when we can.
Obviously, this isn't always possible, but we try.

Rationality should be measured as the average performance measure of an agent over a large number of instances from an environment class.

Table-Driven Agent:
    A table with all possible percept sequences as keys, mapped to actions as values.
    Problems: this can get exponentially large with a large number of values.
    This is completely impossible in a continuous environment.
    But with a simple, discrete environment with few variables, it can provide a good amount of control.
    Cannot deal with unknowns; it can't record what it learned by button-mashing.

Simple Reflex Agent:
    Kind of the opposite of a table driven agent.
    Does not keep track of the entire percept history; makes a decision based on what the environment state is RIGHT NOW.
    Condition-action rules; still uses if-else statements but doesn't rely on large permutations of percept history.
    Example: wasp experiment: a wasp neutralizing its prey to feed to its young, but when researchers drag the prey away it forgets what step of the process it was in. It only acts reflexively based on its immediate environment.
    Having no history can cause problems of its own, but it can be useful in some instances.

Model Based Reflex Agent:
    More sophisticated but more difficult to implement.
    - "What is the world like right now?"
        - Update state 
        - See how the world changes
        - What do my actions do
    - THEN go to condition-action rules and act
    Ex: if the wasp was capable of remembering "i already grabbed that caterpillar, I don't need to keep checking my nest, I can just drag it in there"

Model Based Goal Agent:
    Much more sophisticated but also more difficult.
    The agent is looking forward to a goal; uses what-if scenarios. "What if i did action C followed by action A?"
    It is seeking a specific win condition and finds a way there.
    - "What is the world like right now? What will the world be like next?"
        - Update state 
        - See how the world changes
        - What do my actions do
    - Goals
    - What should I do now?
    More effective at dealing with continuous environments.

Model Based Utility Agent:
    Instead of saying "is this a goal state or not" it RANKS the utility of all potential states.
    - "What is the world like right now? What will the world be like next?"
        - Update state 
        - See how the world changes
        - What do my actions do
    - How happy will I be then?
        - Utility rating
    - What should I do now?

Learning Agent:
    All the agents we have discussed until now CAN be hand-crafted by a developer OR influenced by learning elements.
    Ex: a learning table-driven agent can rewrite its own table as it gains experience, based on the performance measure.
    - Critic -> feedback
    - Learning element provides changes to performance element, and sends learning goals to problem generator
    - Performance element provides knowledge to learning element
    - Problem Generator
    Most of machine learning is embedded beneath this idea.

Ask, "What is the simplest of these agent implementations that will work for my problem?"