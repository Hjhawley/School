The Trolley Problem - a dilemma between ethics and psychology
What we think about what's "right" vs "wrong" depends on
more than just a logical weighing of the pros and cons.
90% of people would pull the lever to save 5 at the expense of
1 life, but only 10% would push 1 person to save 5, even though
they have effectively the same outcome.

We need to decide how to value human life and how we judge what
the "greater good" is.

The trolley problem and its variants can give us a good analogue
for real practical problems that software developers face. A program
has no morals, so we have to make those decisions. When does one
life outweigh another? Whatt's an appropriate threshold for what
is an acceptable risk to human life? How do we make decisions when 
we have incomplete data or unpredictable outcomes?

A specific example is the incident with Tesla's self-driving car.
How did it decide what level of risk was acceptable? How did it
assess risk in the first place? When was it supposed to swerve to
avoid injuring or killing a person? How did it quantify the value
of property compared to the value of a human life? Could that have
played a part in causing the accident?